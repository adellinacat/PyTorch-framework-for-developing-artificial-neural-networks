{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba09af19",
   "metadata": {},
   "source": [
    "### 7. Рекурентные сети для обработки последовательностей\n",
    "Попробуйте обучить нейронную сеть GRU/LSTM для предсказания сентимента сообщений с твитера на примере https://www.kaggle.com/datasets/arkhoshghalb/twitter-sentiment-analysis-hatred-speech\n",
    "\n",
    "Опишите, какой результат вы получили? Что помогло вам улучшить ее точность?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "982f2bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: stop-words in c:\\users\\инна\\appdata\\roaming\\python\\python39\\site-packages (2018.7.23)\n"
     ]
    }
   ],
   "source": [
    "!pip install stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33e91a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: textblob in c:\\users\\инна\\appdata\\roaming\\python\\python39\\site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from textblob) (3.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2022.3.15)\n",
      "Requirement already satisfied: tqdm in c:\\users\\инна\\appdata\\roaming\\python\\python39\\site-packages (from nltk>=3.1->textblob) (4.64.1)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\инна\\appdata\\roaming\\python\\python39\\site-packages (from nltk>=3.1->textblob) (1.3.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\инна\\appdata\\roaming\\python\\python39\\site-packages (from click->nltk>=3.1->textblob) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9e689bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: imblearn in c:\\users\\инна\\appdata\\roaming\\python\\python39\\site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\инна\\appdata\\roaming\\python\\python39\\site-packages (from imblearn) (0.11.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (2.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\инна\\appdata\\roaming\\python\\python39\\site-packages (from imbalanced-learn->imblearn) (1.22.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.7.3)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.0.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\инна\\appdata\\roaming\\python\\python39\\site-packages (from imbalanced-learn->imblearn) (1.3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68ae1ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Инна\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Инна\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Инна\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Инна\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "#from google.colab import drive\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from string import punctuation\n",
    "from textblob import TextBlob, Word\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from itertools import islice\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from stop_words import get_stop_words\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040b4e37",
   "metadata": {},
   "source": [
    "Загрузим данные и посмотрим на них"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e43a9900",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train (7).csv')\n",
    "df_test = pd.read_csv('test (7).csv')\n",
    "df_val = pd.read_csv('val (7).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3efc8699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@alisachachka не уезжаааааааай. :(❤ я тоже не ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @GalyginVadim: Ребята и девчата!\\nВсе в кин...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @ARTEM_KLYUSHIN: Кто ненавидит пробки ретви...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>RT @epupybobv: Хочется котлету по-киевски. Зап...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>@KarineKurganova @Yess__Boss босапопа есбоса н...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text  class\n",
       "0   0  @alisachachka не уезжаааааааай. :(❤ я тоже не ...      0\n",
       "1   1  RT @GalyginVadim: Ребята и девчата!\\nВсе в кин...      1\n",
       "2   2  RT @ARTEM_KLYUSHIN: Кто ненавидит пробки ретви...      0\n",
       "3   3  RT @epupybobv: Хочется котлету по-киевски. Зап...      1\n",
       "4   4  @KarineKurganova @Yess__Boss босапопа есбоса н...      1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35d9738d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>204150</td>\n",
       "      <td>Тектоника и рельеф-самое ужасное в мире мучение(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>204151</td>\n",
       "      <td>Ходили запускать шар желаний, но у нас не полу...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>204152</td>\n",
       "      <td>Хочу лето только ради того, что бы направить н...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>204153</td>\n",
       "      <td>RT @RonyLiss: @colf_ne блин((\\nа я шипперила Ф...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>204154</td>\n",
       "      <td>RT @anna_romt: @ZADROT_PO_IGRAM блин,каждое во...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                               text\n",
       "0  204150   Тектоника и рельеф-самое ужасное в мире мучение(\n",
       "1  204151  Ходили запускать шар желаний, но у нас не полу...\n",
       "2  204152  Хочу лето только ради того, что бы направить н...\n",
       "3  204153  RT @RonyLiss: @colf_ne блин((\\nа я шипперила Ф...\n",
       "4  204154  RT @anna_romt: @ZADROT_PO_IGRAM блин,каждое во..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0829a475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>181467</td>\n",
       "      <td>RT @TukvaSociopat: Максимальный репост! ))) #є...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>181468</td>\n",
       "      <td>чтоб у меня з.п. ежегодно индексировали на инд...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>181469</td>\n",
       "      <td>@chilyandlime нехуя мне не хорошо !!! :((((</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>181470</td>\n",
       "      <td>@inafish нее , когда ногами ахахах когда?ахаха...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>181471</td>\n",
       "      <td>Хочу сделать как лучше,  а получаю как всегда. :(</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                               text  class\n",
       "0  181467  RT @TukvaSociopat: Максимальный репост! ))) #є...      1\n",
       "1  181468  чтоб у меня з.п. ежегодно индексировали на инд...      0\n",
       "2  181469        @chilyandlime нехуя мне не хорошо !!! :((((      0\n",
       "3  181470  @inafish нее , когда ногами ахахах когда?ахаха...      0\n",
       "4  181471  Хочу сделать как лучше,  а получаю как всегда. :(      0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02ffac2",
   "metadata": {},
   "source": [
    "Зададим ряд гиперпараметров, которые будут использоваться в дальнейшем процессе обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfa45844",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 1500\n",
    "max_len = 15\n",
    "num_classes = 1\n",
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4574a1",
   "metadata": {},
   "source": [
    "Предобработка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1079a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = set(get_stop_words(\"ru\"))\n",
    "exclude = set(punctuation)\n",
    "morpher = MorphAnalyzer()\n",
    "\n",
    "def preprocess_text(txt):\n",
    "    txt = str(txt)\n",
    "    txt = \"\".join(c for c in txt if c not in exclude)\n",
    "    txt = txt.lower()\n",
    "    txt = re.sub(\"\\sне\", \"не\", txt)\n",
    "    txt = [morpher.parse(word)[0].normal_form for word in txt.split() if word not in sw]\n",
    "    return \" \".join(txt)\n",
    "\n",
    "df_train['text'] = df_train['text'].apply(preprocess_text)\n",
    "df_val['text'] = df_val['text'].apply(preprocess_text)\n",
    "df_test['text'] = df_test['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d79354d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_words before: 258107\n",
      "num_words after: 66106\n"
     ]
    }
   ],
   "source": [
    "text_corpus_train = df_train['text'].values\n",
    "text_corpus_valid = df_val['text'].values\n",
    "text_corpus_test = df_test['text'].values\n",
    "\n",
    "counts = Counter()\n",
    "for sequence in text_corpus_train:\n",
    "    counts.update(sequence.split())\n",
    "\n",
    "print(\"num_words before:\",len(counts.keys()))\n",
    "for word in list(counts):\n",
    "    if counts[word] < 2:\n",
    "        del counts[word]\n",
    "print(\"num_words after:\",len(counts.keys()))\n",
    "    \n",
    "vocab2index = {\"\":0, \"UNK\":1}\n",
    "words = [\"\", \"UNK\"]\n",
    "for word in counts:\n",
    "    vocab2index[word] = len(words)\n",
    "    words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e791d78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "class TwitterDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, txts, labels, w2index, used_length):\n",
    "        self._txts = txts\n",
    "        self._labels = labels\n",
    "        self._length = used_length\n",
    "        self._w2index = w2index\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._txts)\n",
    "    \n",
    "    @lru_cache(50000)\n",
    "    def encode_sentence(self, txt):\n",
    "        encoded = np.zeros(self._length, dtype=int)\n",
    "        enc1 = np.array([self._w2index.get(word, self._w2index[\"UNK\"]) for word in txt.split()])\n",
    "        length = min(self._length, len(enc1))\n",
    "        encoded[:length] = enc1[:length]\n",
    "        return encoded, length\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        encoded, length = self.encode_sentence(self._txts[index])\n",
    "        return torch.from_numpy(encoded.astype(np.int32)), self._labels[index], length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61528364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(i.split()) for i in text_corpus_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a098c390",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train['class'].values\n",
    "y_val = df_val['class'].values\n",
    "\n",
    "train_dataset = TwitterDataset(text_corpus_train, y_train, vocab2index, 27)\n",
    "valid_dataset = TwitterDataset(text_corpus_valid, y_val, vocab2index, 27)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                          batch_size=128,\n",
    "                          shuffle=True,\n",
    "                          num_workers=3)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset,\n",
    "                          batch_size=128,\n",
    "                          shuffle=False,\n",
    "                          num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4daea455",
   "metadata": {},
   "source": [
    "Инициализируем устройство, на котором будем обучать модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6601a4da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30281f2b",
   "metadata": {},
   "source": [
    "Инициализируем и обучим сеть GRU на данных "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d003dcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "280ce1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUFixedLen(nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim) :\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers=2, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x, l):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        lstm_out, (ht, ct) = self.lstm(x)\n",
    "        return self.linear(lstm_out)\n",
    "    \n",
    "gru_init = GRUFixedLen(len(vocab2index), 30, 20)\n",
    "optimizer = torch.optim.Adam(gru_init.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b39da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Инна\\AppData\\Local\\Temp\\ipykernel_3300\\404843625.py:1: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for epoch in tqdm_notebook(range(10)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49de9d70fbf644d7a39594b1bb3681e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in tqdm_notebook(range(10)):  \n",
    "    gru_init.train()\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels, lengths = data[0], data[1], data[2]\n",
    "        inputs = inputs.long()\n",
    "        labels = labels.long().view(-1, 1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = gru_init(inputs, lengths)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    gru_init.eval()\n",
    "    loss_accumed = 0\n",
    "    for X, y, lengths in valid_loader:\n",
    "        X = X.long()\n",
    "        y = y.long().view(-1, 1)\n",
    "        output = gru_init(X, lengths)\n",
    "        loss = criterion(output, y)\n",
    "        loss_accumed += loss\n",
    "    print(\"Epoch {} valid_loss {}\".format(epoch, loss_accumed))\n",
    "\n",
    "print('Finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10b1229",
   "metadata": {},
   "source": [
    "Посмотрим на результаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5789a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,5))\n",
    "plt.title('Loss history')\n",
    "plt.grid(True)\n",
    "plt.ylabel('Train loss')\n",
    "plt.xlabel('Step')\n",
    "plt.plot(loss_accumed);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b418ebd",
   "metadata": {},
   "source": [
    "Инициализируем и обучим сеть LSTM на данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea09ebf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMFixedLen(nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim) :\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=2, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x, l):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        lstm_out, (ht, ct) = self.lstm(x)\n",
    "        return self.linear(lstm_out)\n",
    "    \n",
    "lstm_init = LSTMFixedLen(len(vocab2index), 30, 20)\n",
    "optimizer = torch.optim.Adam(lstm_init.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9dd48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm_notebook(range(10)):  \n",
    "    gru_init.train()\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels, lengths = data[0], data[1], data[2]\n",
    "        inputs = inputs.long()\n",
    "        labels = labels.long().view(-1, 1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = gru_init(inputs, lengths)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    gru_init.eval()\n",
    "    loss_accumed = 0\n",
    "    for X, y, lengths in valid_loader:\n",
    "        X = X.long()\n",
    "        y = y.long().view(-1, 1)\n",
    "        output = gru_init(X, lengths)\n",
    "        loss = criterion(output, y)\n",
    "        loss_accumed += loss\n",
    "    print(\"Epoch {} valid_loss {}\".format(epoch, loss_accumed))\n",
    "\n",
    "print('Finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2faeaf",
   "metadata": {},
   "source": [
    "Посмотрим на результаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b1b393",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,5))\n",
    "plt.title('Loss history')\n",
    "plt.grid(True)\n",
    "plt.ylabel('Train loss')\n",
    "plt.xlabel('Step')\n",
    "plt.plot(loss_accumed);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676aad01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM с переменным размером входа:\n",
    "class LSTM_variable_input(torch.nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim) :\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, 5)\n",
    "        \n",
    "    def forward(self, x, s):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        x_pack = pack_padded_sequence(x, s, batch_first=True, enforce_sorted=False)\n",
    "        out_pack, (ht, ct) = self.lstm(x_pack)\n",
    "        out = self.linear(ht[-1])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0fc661",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm_notebook(range(10)):  \n",
    "    gru_init.train()\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels, lengths = data[0], data[1], data[2]\n",
    "        inputs = inputs.long()\n",
    "        labels = labels.long().view(-1, 1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = gru_init(inputs, lengths)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    gru_init.eval()\n",
    "    loss_accumed = 0\n",
    "    for X, y, lengths in valid_loader:\n",
    "        X = X.long()\n",
    "        y = y.long().view(-1, 1)\n",
    "        output = gru_init(X, lengths)\n",
    "        loss = criterion(output, y)\n",
    "        loss_accumed += loss\n",
    "    print(\"Epoch {} valid_loss {}\".format(epoch, loss_accumed))\n",
    "\n",
    "print('Finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b81818",
   "metadata": {},
   "source": [
    "Посмотрим на результаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1df6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,5))\n",
    "plt.title('Loss history')\n",
    "plt.grid(True)\n",
    "plt.ylabel('Train loss')\n",
    "plt.xlabel('Step')\n",
    "plt.plot(loss_accumed);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
